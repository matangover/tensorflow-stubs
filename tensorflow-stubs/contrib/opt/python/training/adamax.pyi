# Stubs for tensorflow.contrib.opt.python.training.adamax (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from tensorflow.python.eager import context as context
from tensorflow.python.framework import ops as ops
from tensorflow.python.ops import array_ops as array_ops, control_flow_ops as control_flow_ops, math_ops as math_ops, resource_variable_ops as resource_variable_ops, state_ops as state_ops
from tensorflow.python.training import adam as adam, training_ops as training_ops

class AdaMaxOptimizer(adam.AdamOptimizer):
    def __init__(self, learning_rate: float = ..., beta1: float = ..., beta2: float = ..., epsilon: float = ..., use_locking: bool = ..., name: str = ...) -> None: ...
