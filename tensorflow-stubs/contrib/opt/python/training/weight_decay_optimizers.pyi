# Stubs for tensorflow.contrib.opt.python.training.weight_decay_optimizers (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from tensorflow.contrib.opt.python.training import shampoo as shampoo
from tensorflow.python.framework import ops as ops
from tensorflow.python.ops import array_ops as array_ops, control_flow_ops as control_flow_ops, resource_variable_ops as resource_variable_ops, state_ops as state_ops
from tensorflow.python.training import adam as adam, momentum as momentum_opt, optimizer as optimizer
from tensorflow.python.util.tf_export import tf_export as tf_export
from typing import Any as Any, Optional as Optional

class DecoupledWeightDecayExtension:
    def __init__(self, weight_decay: Any, **kwargs: Any) -> None: ...
    def minimize(self, loss: Any, global_step: Optional[Any] = ..., var_list: Optional[Any] = ..., gate_gradients: Any = ..., aggregation_method: Optional[Any] = ..., colocate_gradients_with_ops: bool = ..., name: Optional[Any] = ..., grad_loss: Optional[Any] = ..., decay_var_list: Optional[Any] = ...): ...
    def apply_gradients(self, grads_and_vars: Any, global_step: Optional[Any] = ..., name: Optional[Any] = ..., decay_var_list: Optional[Any] = ...): ...

def extend_with_decoupled_weight_decay(base_optimizer: Any): ...

class MomentumWOptimizer(DecoupledWeightDecayExtension, momentum_opt.MomentumOptimizer):
    def __init__(self, weight_decay: Any, learning_rate: Any, momentum: Any, use_locking: bool = ..., name: str = ..., use_nesterov: bool = ...) -> None: ...

class AdamWOptimizer(DecoupledWeightDecayExtension, adam.AdamOptimizer):
    def __init__(self, weight_decay: Any, learning_rate: float = ..., beta1: float = ..., beta2: float = ..., epsilon: float = ..., use_locking: bool = ..., name: str = ...) -> None: ...

class ShampooWOptimizer(DecoupledWeightDecayExtension, shampoo.ShampooOptimizer):
    def __init__(self, weight_decay: Any, global_step: Any, max_matrix_size: int = ..., gbar_decay: float = ..., gbar_weight: float = ..., mat_gbar_decay: float = ..., mat_gbar_weight: float = ..., learning_rate: float = ..., svd_interval: int = ..., precond_update_interval: int = ..., epsilon: float = ..., alpha: float = ..., use_iterative_root: bool = ..., use_locking: bool = ..., name: str = ...) -> None: ...
