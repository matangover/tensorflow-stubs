# Stubs for tensorflow.contrib.seq2seq.python.ops.attention_wrapper (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from tensorflow.contrib.framework.python.framework import tensor_util as tensor_util
from tensorflow.python.framework import dtypes as dtypes, ops as ops, tensor_shape as tensor_shape
from tensorflow.python.ops import array_ops as array_ops, check_ops as check_ops, clip_ops as clip_ops, functional_ops as functional_ops, init_ops as init_ops, math_ops as math_ops, nn_ops as nn_ops, random_ops as random_ops, rnn_cell_impl as rnn_cell_impl, tensor_array_ops as tensor_array_ops, variable_scope as variable_scope
from tensorflow.python.util import nest as nest
from typing import Any as Any, Optional as Optional

class AttentionMechanism:
    @property
    def alignments_size(self) -> None: ...
    @property
    def state_size(self) -> None: ...

class _BaseAttentionMechanism(AttentionMechanism):
    dtype: Any = ...
    def __init__(self, query_layer: Any, memory: Any, probability_fn: Any, memory_sequence_length: Optional[Any] = ..., memory_layer: Optional[Any] = ..., check_inner_dims_defined: bool = ..., score_mask_value: Optional[Any] = ..., name: Optional[Any] = ...) -> None: ...
    @property
    def memory_layer(self): ...
    @property
    def query_layer(self): ...
    @property
    def values(self): ...
    @property
    def keys(self): ...
    @property
    def batch_size(self): ...
    @property
    def alignments_size(self): ...
    @property
    def state_size(self): ...
    def initial_alignments(self, batch_size: Any, dtype: Any): ...
    def initial_state(self, batch_size: Any, dtype: Any): ...

class LuongAttention(_BaseAttentionMechanism):
    def __init__(self, num_units: Any, memory: Any, memory_sequence_length: Optional[Any] = ..., scale: bool = ..., probability_fn: Optional[Any] = ..., score_mask_value: Optional[Any] = ..., dtype: Optional[Any] = ..., name: str = ...) -> None: ...
    def __call__(self, query: Any, state: Any): ...

class BahdanauAttention(_BaseAttentionMechanism):
    def __init__(self, num_units: Any, memory: Any, memory_sequence_length: Optional[Any] = ..., normalize: bool = ..., probability_fn: Optional[Any] = ..., score_mask_value: Optional[Any] = ..., dtype: Optional[Any] = ..., name: str = ...) -> None: ...
    def __call__(self, query: Any, state: Any): ...

def safe_cumprod(x: Any, *args: Any, **kwargs: Any): ...
def monotonic_attention(p_choose_i: Any, previous_attention: Any, mode: Any): ...

class _BaseMonotonicAttentionMechanism(_BaseAttentionMechanism):
    def initial_alignments(self, batch_size: Any, dtype: Any): ...

class BahdanauMonotonicAttention(_BaseMonotonicAttentionMechanism):
    def __init__(self, num_units: Any, memory: Any, memory_sequence_length: Optional[Any] = ..., normalize: bool = ..., score_mask_value: Optional[Any] = ..., sigmoid_noise: float = ..., sigmoid_noise_seed: Optional[Any] = ..., score_bias_init: float = ..., mode: str = ..., dtype: Optional[Any] = ..., name: str = ...) -> None: ...
    def __call__(self, query: Any, state: Any): ...

class LuongMonotonicAttention(_BaseMonotonicAttentionMechanism):
    def __init__(self, num_units: Any, memory: Any, memory_sequence_length: Optional[Any] = ..., scale: bool = ..., score_mask_value: Optional[Any] = ..., sigmoid_noise: float = ..., sigmoid_noise_seed: Optional[Any] = ..., score_bias_init: float = ..., mode: str = ..., dtype: Optional[Any] = ..., name: str = ...) -> None: ...
    def __call__(self, query: Any, state: Any): ...

class AttentionWrapperState:
    def clone(self, **kwargs: Any): ...

def hardmax(logits: Any, name: Optional[Any] = ...): ...

class AttentionWrapper(rnn_cell_impl.RNNCell):
    def __init__(self, cell: Any, attention_mechanism: Any, attention_layer_size: Optional[Any] = ..., alignment_history: bool = ..., cell_input_fn: Optional[Any] = ..., output_attention: bool = ..., initial_cell_state: Optional[Any] = ..., name: Optional[Any] = ..., attention_layer: Optional[Any] = ...) -> None: ...
    @property
    def output_size(self): ...
    @property
    def state_size(self): ...
    def zero_state(self, batch_size: Any, dtype: Any): ...
    def call(self, inputs: Any, state: Any): ...
