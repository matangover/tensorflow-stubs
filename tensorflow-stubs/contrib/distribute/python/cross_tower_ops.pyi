# Stubs for tensorflow.contrib.distribute.python.cross_tower_ops (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from collections import namedtuple as namedtuple
from tensorflow.contrib.distribute.python import cross_tower_utils as cross_tower_utils
from tensorflow.python.client import device_lib as device_lib
from tensorflow.python.eager import context as context
from tensorflow.python.framework import ops as ops
from tensorflow.python.ops import array_ops as array_ops, math_ops as math_ops, resource_variable_ops as resource_variable_ops
from tensorflow.python.training import device_util as device_util
from typing import Any as Any, Optional as Optional

def check_destinations(destinations: Any): ...
def validate_destinations(destinations: Any) -> None: ...
def get_devices_from(destinations: Any): ...

class CrossTowerOps:
    def __init__(self) -> None: ...
    def reduce(self, aggregation: Any, per_device_value: Any, destinations: Any): ...
    def batch_reduce(self, aggregation: Any, value_destination_pairs: Any): ...
    def broadcast(self, tensor: Any, destinations: Any): ...

class ReductionToOneDeviceCrossTowerOps(CrossTowerOps):
    reduce_to_device: Any = ...
    accumulation_fn: Any = ...
    def __init__(self, reduce_to_device: Optional[Any] = ..., accumulation_fn: Any = ...) -> None: ...

class ConcatAndSplitPacker:
    num_packs: Any = ...
    def __init__(self, num_packs: int = ...) -> None: ...
    grouped_grads_and_vars: Any = ...
    all_tower_shapes: Any = ...
    all_tower_sizes: Any = ...
    def pack(self, grouped_grads_and_vars: Any): ...
    def unpack(self, summed_device_grad_packs: Any): ...

class AggregateSmallTensorPacker:
    agg_small_grads_max_bytes: Any = ...
    agg_small_grads_max_group: Any = ...
    def __init__(self, agg_small_grads_max_bytes: int = ..., agg_small_grads_max_group: int = ...) -> None: ...
    def pack(self, grouped_grads_and_vars: Any): ...
    def unpack(self, summed_device_grad_packs: Any): ...

class AllReduceCrossTowerOps(CrossTowerOps):
    def __init__(self, all_reduce_alg: str = ..., num_packs: int = ..., agg_small_grads_max_bytes: int = ..., agg_small_grads_max_group: int = ...) -> None: ...

AllReduceSpecTuple = namedtuple('AllReduceSpecTuple', 'alg shards limit')

class MultiWorkerAllReduce(AllReduceCrossTowerOps):
    def __init__(self, worker_devices: Any, num_gpus_per_worker: Any, all_reduce_spec: Any = ..., num_packs: int = ..., agg_small_grads_max_bytes: int = ..., agg_small_grads_max_group: int = ...) -> None: ...

class CollectiveAllReduce(CrossTowerOps):
    def __init__(self, num_workers: int = ..., num_gpus_per_worker: int = ..., all_reduce_merge_scope: int = ..., collective_keys: Optional[Any] = ...) -> None: ...

def choose_the_best(devices: Any, session_config: Optional[Any] = ...): ...
