# Stubs for tensorflow.python.keras.backend (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from tensorflow.core.protobuf import config_pb2 as config_pb2
from tensorflow.python.eager import context as context
from tensorflow.python.framework import constant_op as constant_op, ops as ops, sparse_tensor as sparse_tensor, tensor_util as tensor_util
from tensorflow.python.ops import array_ops as array_ops, clip_ops as clip_ops, control_flow_ops as control_flow_ops, functional_ops as functional_ops, image_ops as image_ops, init_ops as init_ops, linalg_ops as linalg_ops, logging_ops as logging_ops, math_ops as math_ops, nn as nn, random_ops as random_ops, resource_variable_ops as resource_variable_ops, sparse_ops as sparse_ops, state_ops as state_ops, tensor_array_grad as tensor_array_grad, tensor_array_ops as tensor_array_ops
from tensorflow.python.util import tf_contextlib as tf_contextlib, tf_inspect as tf_inspect
from tensorflow.python.util.tf_export import tf_export as tf_export
from typing import Any as Any, Optional as Optional

py_all = all
py_sum = sum

class _DummyEagerGraph: ...

def backend(): ...
def epsilon(): ...
def set_epsilon(value: Any) -> None: ...
def floatx(): ...
def set_floatx(value: Any) -> None: ...
def cast_to_floatx(x: Any): ...
def image_data_format(): ...
def set_image_data_format(data_format: Any) -> None: ...

PER_GRAPH_LAYER_NAME_UIDS: Any

def get_uid(prefix: str = ...): ...
def reset_uids() -> None: ...
def clear_session() -> None: ...
def manual_variable_initialization(value: Any) -> None: ...
def learning_phase(): ...
def set_learning_phase(value: Any) -> None: ...
def learning_phase_scope(value: Any) -> None: ...
def get_session(): ...
def set_session(session: Any) -> None: ...
def get_default_session_config(): ...

class _TfDeviceCaptureOp:
    device: Any = ...
    def __init__(self) -> None: ...

def is_sparse(tensor: Any): ...
def to_dense(tensor: Any): ...
name_scope = ops.name_scope

def variable(value: Any, dtype: Optional[Any] = ..., name: Optional[Any] = ..., constraint: Optional[Any] = ...): ...
def track_tf_optimizer(tf_optimizer: Any) -> None: ...
def track_variable(v: Any) -> None: ...
def constant(value: Any, dtype: Optional[Any] = ..., shape: Optional[Any] = ..., name: Optional[Any] = ...): ...
def is_keras_tensor(x: Any): ...
def placeholder(shape: Optional[Any] = ..., ndim: Optional[Any] = ..., dtype: Optional[Any] = ..., sparse: bool = ..., name: Optional[Any] = ...): ...
def is_placeholder(x: Any): ...
def shape(x: Any): ...
def int_shape(x: Any): ...
def ndim(x: Any): ...
def dtype(x: Any): ...
def eval(x: Any): ...
def zeros(shape: Any, dtype: Optional[Any] = ..., name: Optional[Any] = ...): ...
def ones(shape: Any, dtype: Optional[Any] = ..., name: Optional[Any] = ...): ...
def eye(size: Any, dtype: Optional[Any] = ..., name: Optional[Any] = ...): ...
def zeros_like(x: Any, dtype: Optional[Any] = ..., name: Optional[Any] = ...): ...
def ones_like(x: Any, dtype: Optional[Any] = ..., name: Optional[Any] = ...): ...
def identity(x: Any, name: Optional[Any] = ...): ...
def random_uniform_variable(shape: Any, low: Any, high: Any, dtype: Optional[Any] = ..., name: Optional[Any] = ..., seed: Optional[Any] = ...): ...
def random_normal_variable(shape: Any, mean: Any, scale: Any, dtype: Optional[Any] = ..., name: Optional[Any] = ..., seed: Optional[Any] = ...): ...
def count_params(x: Any): ...
def cast(x: Any, dtype: Any): ...
def update(x: Any, new_x: Any): ...
def update_add(x: Any, increment: Any): ...
def update_sub(x: Any, decrement: Any): ...
def moving_average_update(x: Any, value: Any, momentum: Any): ...
def dot(x: Any, y: Any): ...
def batch_dot(x: Any, y: Any, axes: Optional[Any] = ...): ...
def transpose(x: Any): ...
def gather(reference: Any, indices: Any): ...
def max(x: Any, axis: Optional[Any] = ..., keepdims: bool = ...): ...
def min(x: Any, axis: Optional[Any] = ..., keepdims: bool = ...): ...
def sum(x: Any, axis: Optional[Any] = ..., keepdims: bool = ...): ...
def prod(x: Any, axis: Optional[Any] = ..., keepdims: bool = ...): ...
def cumsum(x: Any, axis: int = ...): ...
def cumprod(x: Any, axis: int = ...): ...
def var(x: Any, axis: Optional[Any] = ..., keepdims: bool = ...): ...
def std(x: Any, axis: Optional[Any] = ..., keepdims: bool = ...): ...
def mean(x: Any, axis: Optional[Any] = ..., keepdims: bool = ...): ...
def any(x: Any, axis: Optional[Any] = ..., keepdims: bool = ...): ...
def all(x: Any, axis: Optional[Any] = ..., keepdims: bool = ...): ...
def argmax(x: Any, axis: int = ...): ...
def argmin(x: Any, axis: int = ...): ...
def square(x: Any): ...
def abs(x: Any): ...
def sqrt(x: Any): ...
def exp(x: Any): ...
def log(x: Any): ...
def logsumexp(x: Any, axis: Optional[Any] = ..., keepdims: bool = ...): ...
def round(x: Any): ...
def sign(x: Any): ...
def pow(x: Any, a: Any): ...
def clip(x: Any, min_value: Any, max_value: Any): ...
def equal(x: Any, y: Any): ...
def not_equal(x: Any, y: Any): ...
def greater(x: Any, y: Any): ...
def greater_equal(x: Any, y: Any): ...
def less(x: Any, y: Any): ...
def less_equal(x: Any, y: Any): ...
def maximum(x: Any, y: Any): ...
def minimum(x: Any, y: Any): ...
def sin(x: Any): ...
def cos(x: Any): ...
def normalize_batch_in_training(x: Any, gamma: Any, beta: Any, reduction_axes: Any, epsilon: float = ...): ...
def batch_normalization(x: Any, mean: Any, var: Any, beta: Any, gamma: Any, epsilon: float = ...): ...
def concatenate(tensors: Any, axis: int = ...): ...
def reshape(x: Any, shape: Any): ...
def permute_dimensions(x: Any, pattern: Any): ...
def resize_images(x: Any, height_factor: Any, width_factor: Any, data_format: Any): ...
def resize_volumes(x: Any, depth_factor: Any, height_factor: Any, width_factor: Any, data_format: Any): ...
def repeat_elements(x: Any, rep: Any, axis: Any): ...
def repeat(x: Any, n: Any): ...
def arange(start: Any, stop: Optional[Any] = ..., step: int = ..., dtype: str = ...): ...
def tile(x: Any, n: Any): ...
def flatten(x: Any): ...
def batch_flatten(x: Any): ...
def expand_dims(x: Any, axis: int = ...): ...
def squeeze(x: Any, axis: Any): ...
def temporal_padding(x: Any, padding: Any = ...): ...
def spatial_2d_padding(x: Any, padding: Any = ..., data_format: Optional[Any] = ...): ...
def spatial_3d_padding(x: Any, padding: Any = ..., data_format: Optional[Any] = ...): ...
def stack(x: Any, axis: int = ...): ...
def one_hot(indices: Any, num_classes: Any): ...
def reverse(x: Any, axes: Any): ...
def get_value(x: Any): ...
def batch_get_value(tensors: Any): ...
def set_value(x: Any, value: Any) -> None: ...
def batch_set_value(tuples: Any) -> None: ...
def print_tensor(x: Any, message: str = ...): ...

class Function:
    inputs: Any = ...
    outputs: Any = ...
    updates_op: Any = ...
    name: Any = ...
    feed_dict: Any = ...
    fetches: Any = ...
    run_options: Any = ...
    run_metadata: Any = ...
    session_kwargs: Any = ...
    fetch_callbacks: Any = ...
    def __init__(self, inputs: Any, outputs: Any, updates: Optional[Any] = ..., name: Optional[Any] = ..., **session_kwargs: Any) -> None: ...
    def __call__(self, inputs: Any): ...

def function(inputs: Any, outputs: Any, updates: Optional[Any] = ..., **kwargs: Any): ...
def gradients(loss: Any, variables: Any): ...
def stop_gradient(variables: Any): ...
def rnn(step_function: Any, inputs: Any, initial_states: Any, go_backwards: bool = ..., mask: Optional[Any] = ..., constants: Optional[Any] = ..., unroll: bool = ..., input_length: Optional[Any] = ...): ...
def switch(condition: Any, then_expression: Any, else_expression: Any): ...
def in_train_phase(x: Any, alt: Any, training: Optional[Any] = ...): ...
def in_test_phase(x: Any, alt: Any, training: Optional[Any] = ...): ...
def relu(x: Any, alpha: float = ..., max_value: Optional[Any] = ..., threshold: int = ...): ...
def elu(x: Any, alpha: float = ...): ...
def softmax(x: Any, axis: int = ...): ...
def softplus(x: Any): ...
def softsign(x: Any): ...
def categorical_crossentropy(target: Any, output: Any, from_logits: bool = ..., axis: int = ...): ...
def sparse_categorical_crossentropy(target: Any, output: Any, from_logits: bool = ..., axis: int = ...): ...
def binary_crossentropy(target: Any, output: Any, from_logits: bool = ...): ...
def sigmoid(x: Any): ...
def hard_sigmoid(x: Any): ...
def tanh(x: Any): ...
def dropout(x: Any, level: Any, noise_shape: Optional[Any] = ..., seed: Optional[Any] = ...): ...
def l2_normalize(x: Any, axis: Optional[Any] = ...): ...
def in_top_k(predictions: Any, targets: Any, k: Any): ...
def conv1d(x: Any, kernel: Any, strides: int = ..., padding: str = ..., data_format: Optional[Any] = ..., dilation_rate: int = ...): ...
def conv2d(x: Any, kernel: Any, strides: Any = ..., padding: str = ..., data_format: Optional[Any] = ..., dilation_rate: Any = ...): ...
def conv2d_transpose(x: Any, kernel: Any, output_shape: Any, strides: Any = ..., padding: str = ..., data_format: Optional[Any] = ...): ...
def separable_conv1d(x: Any, depthwise_kernel: Any, pointwise_kernel: Any, strides: int = ..., padding: str = ..., data_format: Optional[Any] = ..., dilation_rate: int = ...): ...
def separable_conv2d(x: Any, depthwise_kernel: Any, pointwise_kernel: Any, strides: Any = ..., padding: str = ..., data_format: Optional[Any] = ..., dilation_rate: Any = ...): ...
def depthwise_conv2d(x: Any, depthwise_kernel: Any, strides: Any = ..., padding: str = ..., data_format: Optional[Any] = ..., dilation_rate: Any = ...): ...
def conv3d(x: Any, kernel: Any, strides: Any = ..., padding: str = ..., data_format: Optional[Any] = ..., dilation_rate: Any = ...): ...
def conv3d_transpose(x: Any, kernel: Any, output_shape: Any, strides: Any = ..., padding: str = ..., data_format: Optional[Any] = ...): ...
def pool2d(x: Any, pool_size: Any, strides: Any = ..., padding: str = ..., data_format: Optional[Any] = ..., pool_mode: str = ...): ...
def pool3d(x: Any, pool_size: Any, strides: Any = ..., padding: str = ..., data_format: Optional[Any] = ..., pool_mode: str = ...): ...
def local_conv(inputs: Any, kernel: Any, kernel_size: Any, strides: Any, output_shape: Any, data_format: Optional[Any] = ...): ...
def local_conv1d(inputs: Any, kernel: Any, kernel_size: Any, strides: Any, data_format: Optional[Any] = ...): ...
def local_conv2d(inputs: Any, kernel: Any, kernel_size: Any, strides: Any, output_shape: Any, data_format: Optional[Any] = ...): ...
def bias_add(x: Any, bias: Any, data_format: Optional[Any] = ...): ...
def random_normal(shape: Any, mean: float = ..., stddev: float = ..., dtype: Optional[Any] = ..., seed: Optional[Any] = ...): ...
def random_uniform(shape: Any, minval: float = ..., maxval: float = ..., dtype: Optional[Any] = ..., seed: Optional[Any] = ...): ...
def random_binomial(shape: Any, p: float = ..., dtype: Optional[Any] = ..., seed: Optional[Any] = ...): ...
def truncated_normal(shape: Any, mean: float = ..., stddev: float = ..., dtype: Optional[Any] = ..., seed: Optional[Any] = ...): ...
def ctc_label_dense_to_sparse(labels: Any, label_lengths: Any): ...
def ctc_batch_cost(y_true: Any, y_pred: Any, input_length: Any, label_length: Any): ...
def ctc_decode(y_pred: Any, input_length: Any, greedy: bool = ..., beam_width: int = ..., top_paths: int = ...): ...
def map_fn(fn: Any, elems: Any, name: Optional[Any] = ..., dtype: Optional[Any] = ...): ...
def foldl(fn: Any, elems: Any, initializer: Optional[Any] = ..., name: Optional[Any] = ...): ...
def foldr(fn: Any, elems: Any, initializer: Optional[Any] = ..., name: Optional[Any] = ...): ...
