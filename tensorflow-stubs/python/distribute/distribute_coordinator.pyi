# Stubs for tensorflow.python.distribute.distribute_coordinator (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from tensorflow.core.protobuf import config_pb2 as config_pb2
from tensorflow.python.client import session as session
from tensorflow.python.distribute import distribute_coordinator_context as distribute_coordinator_context, multi_worker_util as multi_worker_util
from tensorflow.python.training import monitored_session as monitored_session, server_lib as server_lib
from typing import Any as Any, Optional as Optional

class _TaskType:
    PS: str = ...
    WORKER: str = ...
    CHIEF: str = ...
    EVALUATOR: str = ...
    CLIENT: str = ...

class CoordinatorMode:
    STANDALONE_CLIENT: str = ...
    INDEPENDENT_WORKER: str = ...

class _Barrier:
    def __init__(self, num_participants: Any) -> None: ...
    def wait(self) -> None: ...

class _WorkerContext:
    def __init__(self, strategy: Any, cluster_spec: Any, task_type: Any, task_id: Any, session_config: Optional[Any] = ..., rpc_layer: str = ..., worker_barrier: Optional[Any] = ...) -> None: ...
    def __enter__(self) -> None: ...
    def __exit__(self, unused_exception_type: Any, unused_exception_value: Any, unused_traceback: Any) -> None: ...
    def wait_for_other_workers(self) -> None: ...
    def session_creator(self, scaffold: Optional[Any] = ..., config: Optional[Any] = ..., checkpoint_dir: Optional[Any] = ..., checkpoint_filename_with_path: Optional[Any] = ..., max_wait_secs: int = ...): ...
    @property
    def has_barrier(self): ...
    @property
    def distributed_mode(self): ...
    @property
    def cluster_spec(self): ...
    @property
    def task_type(self): ...
    @property
    def task_id(self): ...
    @property
    def master_target(self): ...
    @property
    def is_chief(self): ...
    @property
    def num_workers(self): ...
    @property
    def should_checkpoint(self): ...
    @property
    def should_save_summary(self): ...

def run_standard_tensorflow_server(session_config: Optional[Any] = ...): ...
def run_distribute_coordinator(worker_fn: Any, strategy: Any, eval_fn: Optional[Any] = ..., eval_strategy: Optional[Any] = ..., mode: Any = ..., cluster_spec: Optional[Any] = ..., task_type: Optional[Any] = ..., task_id: Optional[Any] = ..., session_config: Optional[Any] = ..., rpc_layer: str = ...) -> None: ...
