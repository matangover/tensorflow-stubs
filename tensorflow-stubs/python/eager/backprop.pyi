# Stubs for tensorflow.python.eager.backprop (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from tensorflow.python import pywrap_tensorflow as pywrap_tensorflow
from tensorflow.python.eager import context as context, execute as execute, imperative_grad as imperative_grad, tape as tape
from tensorflow.python.framework import constant_op as constant_op, dtypes as dtypes, ops as ops, tensor_shape as tensor_shape
from tensorflow.python.ops import array_ops as array_ops, gen_array_ops as gen_array_ops, gen_math_ops as gen_math_ops, math_ops as math_ops, resource_variable_ops as resource_variable_ops
from tensorflow.python.util import nest as nest, tf_contextlib as tf_contextlib, tf_inspect as tf_inspect
from tensorflow.python.util.tf_export import tf_export as tf_export
from typing import Any as Any, Optional as Optional

def op_attr_type(op_type: Any, attr_name: Any): ...
def make_attr(attr_type: Any, value: Any): ...

class _MockOp:
    attrs: Any = ...
    inputs: Any = ...
    outputs: Any = ...
    type: Any = ...
    def __init__(self, attrs: Any, inputs: Any, outputs: Any, typ: Any) -> None: ...
    def get_attr(self, attr: Any): ...

def implicit_val_and_grad(f: Any): ...
def implicit_grad(f: Any): ...
def gradients_function(f: Any, params: Optional[Any] = ...): ...
def val_and_grad_function(f: Any, params: Optional[Any] = ...): ...
def make_vjp(f: Any, params: Optional[Any] = ..., persistent: bool = ...): ...

class GradientTape:
    def __init__(self, persistent: bool = ..., watch_accessed_variables: bool = ...) -> None: ...
    def __enter__(self): ...
    def __exit__(self, typ: Any, value: Any, traceback: Any) -> None: ...
    def __del__(self) -> None: ...
    def watch(self, tensor: Any) -> None: ...
    def stop_recording(self) -> None: ...
    def reset(self) -> None: ...
    def watched_variables(self): ...
    def gradient(self, target: Any, sources: Any, output_gradients: Optional[Any] = ...): ...
