# Stubs for tensorflow.python.keras.layers.advanced_activations (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from tensorflow.python.keras import constraints as constraints, initializers as initializers, regularizers as regularizers
from tensorflow.python.keras.engine.base_layer import InputSpec as InputSpec, Layer as Layer
from tensorflow.python.keras.utils import tf_utils as tf_utils
from tensorflow.python.ops import math_ops as math_ops
from tensorflow.python.util.tf_export import tf_export as tf_export
from typing import Any as Any, Optional as Optional

class LeakyReLU(Layer):
    supports_masking: bool = ...
    alpha: Any = ...
    def __init__(self, alpha: float = ..., **kwargs: Any) -> None: ...
    def call(self, inputs: Any): ...
    def get_config(self): ...
    def compute_output_shape(self, input_shape: Any): ...

class PReLU(Layer):
    supports_masking: bool = ...
    alpha_initializer: Any = ...
    alpha_regularizer: Any = ...
    alpha_constraint: Any = ...
    shared_axes: Any = ...
    def __init__(self, alpha_initializer: str = ..., alpha_regularizer: Optional[Any] = ..., alpha_constraint: Optional[Any] = ..., shared_axes: Optional[Any] = ..., **kwargs: Any) -> None: ...
    param_broadcast: Any = ...
    alpha: Any = ...
    input_spec: Any = ...
    built: bool = ...
    def build(self, input_shape: Any) -> None: ...
    def call(self, inputs: Any, mask: Optional[Any] = ...): ...
    def get_config(self): ...
    def compute_output_shape(self, input_shape: Any): ...

class ELU(Layer):
    supports_masking: bool = ...
    alpha: Any = ...
    def __init__(self, alpha: float = ..., **kwargs: Any) -> None: ...
    def call(self, inputs: Any): ...
    def get_config(self): ...
    def compute_output_shape(self, input_shape: Any): ...

class ThresholdedReLU(Layer):
    supports_masking: bool = ...
    theta: Any = ...
    def __init__(self, theta: float = ..., **kwargs: Any) -> None: ...
    def call(self, inputs: Any, mask: Optional[Any] = ...): ...
    def get_config(self): ...
    def compute_output_shape(self, input_shape: Any): ...

class Softmax(Layer):
    supports_masking: bool = ...
    axis: Any = ...
    def __init__(self, axis: int = ..., **kwargs: Any) -> None: ...
    def call(self, inputs: Any): ...
    def get_config(self): ...
    def compute_output_shape(self, input_shape: Any): ...

class ReLU(Layer):
    support_masking: bool = ...
    max_value: Any = ...
    negative_slope: Any = ...
    threshold: Any = ...
    def __init__(self, max_value: Optional[Any] = ..., negative_slope: int = ..., threshold: int = ..., **kwargs: Any) -> None: ...
    def call(self, inputs: Any): ...
    def get_config(self): ...
    def compute_output_shape(self, input_shape: Any): ...
