# Stubs for tensorflow.python.estimator.canned.dnn (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from tensorflow.python.estimator import estimator as estimator, model_fn as model_fn
from tensorflow.python.estimator.canned import optimizers as optimizers
from tensorflow.python.feature_column import feature_column as feature_column, feature_column_v2 as feature_column_v2
from tensorflow.python.framework import ops as ops
from tensorflow.python.keras.engine import training as training
from tensorflow.python.layers import normalization as normalization
from tensorflow.python.ops import init_ops as init_ops, nn as nn, partitioned_variables as partitioned_variables, variable_scope as variable_scope
from tensorflow.python.ops.losses import losses as losses
from tensorflow.python.summary import summary as summary
from tensorflow.python.util.tf_export import estimator_export as estimator_export
from typing import Any as Any, Optional as Optional

class _DNNModel(training.Model):
    def __init__(self, units: Any, hidden_units: Any, feature_columns: Any, activation_fn: Any, dropout: Any, input_layer_partitioner: Any, batch_norm: Any, shared_state_manager: Any, name: Optional[Any] = ..., **kwargs: Any) -> None: ...
    def call(self, features: Any, mode: Any): ...

class DNNClassifier(estimator.Estimator):
    def __init__(self, hidden_units: Any, feature_columns: Any, model_dir: Optional[Any] = ..., n_classes: int = ..., weight_column: Optional[Any] = ..., label_vocabulary: Optional[Any] = ..., optimizer: str = ..., activation_fn: Any = ..., dropout: Optional[Any] = ..., input_layer_partitioner: Optional[Any] = ..., config: Optional[Any] = ..., warm_start_from: Optional[Any] = ..., loss_reduction: Any = ..., batch_norm: bool = ...) -> None: ...

class DNNRegressor(estimator.Estimator):
    def __init__(self, hidden_units: Any, feature_columns: Any, model_dir: Optional[Any] = ..., label_dimension: int = ..., weight_column: Optional[Any] = ..., optimizer: str = ..., activation_fn: Any = ..., dropout: Optional[Any] = ..., input_layer_partitioner: Optional[Any] = ..., config: Optional[Any] = ..., warm_start_from: Optional[Any] = ..., loss_reduction: Any = ..., batch_norm: bool = ...) -> None: ...
