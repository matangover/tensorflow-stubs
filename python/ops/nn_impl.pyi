# Stubs for tensorflow.python.ops.nn_impl (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from tensorflow.python.framework import constant_op as constant_op, dtypes as dtypes, function as function, ops as ops
from tensorflow.python.ops import array_ops as array_ops, candidate_sampling_ops as candidate_sampling_ops, embedding_ops as embedding_ops, gen_array_ops as gen_array_ops, gen_nn_ops as gen_nn_ops, math_ops as math_ops, nn_ops as nn_ops, sparse_ops as sparse_ops, variables as variables
from tensorflow.python.util.deprecation import deprecated_args as deprecated_args, deprecated_argument_lookup as deprecated_argument_lookup
from tensorflow.python.util.tf_export import tf_export as tf_export
from typing import Any as Any, Optional as Optional

def log_poisson_loss(targets: Any, log_input: Any, compute_full_loss: bool = ..., name: Optional[Any] = ...): ...
def sigmoid_cross_entropy_with_logits(_sentinel: Optional[Any] = ..., labels: Optional[Any] = ..., logits: Optional[Any] = ..., name: Optional[Any] = ...): ...
def weighted_cross_entropy_with_logits(targets: Any, logits: Any, pos_weight: Any, name: Optional[Any] = ...): ...
def relu_layer(x: Any, weights: Any, biases: Any, name: Optional[Any] = ...): ...
def swish(features: Any): ...
def l2_normalize(x: Any, axis: Optional[Any] = ..., epsilon: float = ..., name: Optional[Any] = ..., dim: Optional[Any] = ...): ...
def zero_fraction(value: Any, name: Optional[Any] = ...): ...
def depthwise_conv2d(input: Any, filter: Any, strides: Any, padding: Any, rate: Optional[Any] = ..., name: Optional[Any] = ..., data_format: Optional[Any] = ...): ...
def separable_conv2d(input: Any, depthwise_filter: Any, pointwise_filter: Any, strides: Any, padding: Any, rate: Optional[Any] = ..., name: Optional[Any] = ..., data_format: Optional[Any] = ...): ...
def sufficient_statistics(x: Any, axes: Any, shift: Optional[Any] = ..., keep_dims: bool = ..., name: Optional[Any] = ...): ...
def normalize_moments(counts: Any, mean_ss: Any, variance_ss: Any, shift: Any, name: Optional[Any] = ...): ...
def moments(x: Any, axes: Any, shift: Optional[Any] = ..., name: Optional[Any] = ..., keep_dims: bool = ...): ...
def weighted_moments(x: Any, axes: Any, frequency_weights: Any, name: Optional[Any] = ..., keep_dims: bool = ...): ...
def batch_normalization(x: Any, mean: Any, variance: Any, offset: Any, scale: Any, variance_epsilon: Any, name: Optional[Any] = ...): ...
def fused_batch_norm(x: Any, scale: Any, offset: Any, mean: Optional[Any] = ..., variance: Optional[Any] = ..., epsilon: float = ..., data_format: str = ..., is_training: bool = ..., name: Optional[Any] = ...): ...
def batch_norm_with_global_normalization(t: Any, m: Any, v: Any, beta: Any, gamma: Any, variance_epsilon: Any, scale_after_normalization: Any, name: Optional[Any] = ...): ...
def nce_loss(weights: Any, biases: Any, labels: Any, inputs: Any, num_sampled: Any, num_classes: Any, num_true: int = ..., sampled_values: Optional[Any] = ..., remove_accidental_hits: bool = ..., partition_strategy: str = ..., name: str = ...): ...
def sampled_softmax_loss(weights: Any, biases: Any, labels: Any, inputs: Any, num_sampled: Any, num_classes: Any, num_true: int = ..., sampled_values: Optional[Any] = ..., remove_accidental_hits: bool = ..., partition_strategy: str = ..., name: str = ..., seed: Optional[Any] = ...): ...
