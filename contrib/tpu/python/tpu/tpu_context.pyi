# Stubs for tensorflow.contrib.tpu.python.tpu.tpu_context (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from tensorflow.contrib.tpu.python.tpu import tpu_config as tpu_config
from typing import Any as Any, Optional as Optional

class TPUContext:
    def __init__(self, internal_ctx: Any, input_device: Optional[Any] = ..., invocation_index: Optional[Any] = ..., call_from_input_fn: bool = ...) -> None: ...
    def current_input_fn_deployment(self): ...
    @property
    def num_replicas(self): ...
    @property
    def num_hosts(self): ...
    @property
    def current_host(self): ...
    @property
    def num_of_replicas_per_host(self): ...
    @property
    def device_assignment(self): ...
    def device_for_replica(self, replica_id: Any): ...
    @property
    def tpu_host_placement_function(self): ...

class _InternalTPUContext:
    def __init__(self, config: Any, train_batch_size: Any, eval_batch_size: Any, predict_batch_size: Any, use_tpu: Any, eval_on_tpu: bool = ...) -> None: ...
    def with_mode(self, mode: Any) -> None: ...
    @property
    def mode(self): ...
    @property
    def model_parallelism_enabled(self): ...
    @property
    def input_partition_dims(self): ...
    @property
    def device_assignment(self): ...
    @property
    def num_of_cores_per_host(self): ...
    @property
    def num_cores(self): ...
    @property
    def num_of_replicas_per_host(self): ...
    @property
    def num_replicas(self): ...
    @property
    def num_hosts(self): ...
    @property
    def config(self): ...
    def is_input_sharded_per_core(self): ...
    def is_input_per_host_with_iterators(self): ...
    def is_input_broadcast_with_iterators(self): ...
    def is_running_on_cpu(self, is_export_mode: bool = ...): ...
    @property
    def global_batch_size(self): ...
    @property
    def batch_size_for_input_fn(self): ...
    @property
    def batch_size_for_model_fn(self): ...
    @property
    def master_job(self): ...
    @property
    def tpu_host_placement_function(self): ...
    @property
    def tpu_device_placement_function(self): ...
    def tpu_ordinal_function(self, host_id: Any): ...
    def device_for_replica(self, replica_id: Any): ...

class _OneCoreTPUContext(_InternalTPUContext):
    def __init__(self, config: Any, train_batch_size: Any, eval_batch_size: Any, predict_batch_size: Any, use_tpu: Any) -> None: ...
