# Stubs for tensorflow.contrib.training.python.training.training (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

from tensorflow.python.framework import constant_op as constant_op, ops as ops
from tensorflow.python.ops import array_ops as array_ops, clip_ops as clip_ops, control_flow_ops as control_flow_ops
from tensorflow.python.summary import summary as summary
from tensorflow.python.training import monitored_session as monitored_session, training_util as training_util
from typing import Any as Any, Optional as Optional

def add_gradients_summaries(grads_and_vars: Any): ...
def clip_gradient_norms(gradients_to_variables: Any, max_norm: Any): ...
def clip_gradient_norms_fn(max_norm: Any): ...
def multiply_gradients(grads_and_vars: Any, gradient_multipliers: Any): ...
def create_train_op(total_loss: Any, optimizer: Any, global_step: Any = ..., update_ops: Optional[Any] = ..., variables_to_train: Optional[Any] = ..., transform_grads_fn: Optional[Any] = ..., summarize_gradients: bool = ..., gate_gradients: Any = ..., aggregation_method: Optional[Any] = ..., colocate_gradients_with_ops: bool = ..., check_numerics: bool = ...): ...
def train(train_op: Any, logdir: Any, master: str = ..., is_chief: bool = ..., scaffold: Optional[Any] = ..., hooks: Optional[Any] = ..., chief_only_hooks: Optional[Any] = ..., save_checkpoint_secs: int = ..., save_summaries_steps: int = ..., config: Optional[Any] = ..., max_wait_secs: int = ..., run_metadata: Optional[Any] = ...): ...
