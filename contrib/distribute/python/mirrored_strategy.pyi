# Stubs for tensorflow.contrib.distribute.python.mirrored_strategy (Python 3)
#
# NOTE: This dynamically typed stub was automatically generated by stubgen.

import threading as threading
from tensorflow.contrib.distribute.python import shared_variable_creator as shared_variable_creator, values as values
from tensorflow.python import pywrap_tensorflow as pywrap_tensorflow
from tensorflow.python.distribute import multi_worker_util as multi_worker_util
from tensorflow.python.eager import context as context, tape as tape
from tensorflow.python.framework import constant_op as constant_op, ops as ops
from tensorflow.python.ops import array_ops as array_ops, control_flow_ops as control_flow_ops, variable_scope as variable_scope
from tensorflow.python.training import coordinator as coordinator, device_util as device_util, distribute as distribute_lib
from tensorflow.python.util import nest as nest
from typing import Any as Any, Optional as Optional

class _RequestedStop(Exception): ...

class MirroredStrategy(distribute_lib.DistributionStrategy):
    def __init__(self, devices: Optional[Any] = ..., num_gpus: Optional[Any] = ..., num_gpus_per_worker: Optional[Any] = ..., cross_tower_ops: Optional[Any] = ..., prefetch_on_device: Optional[Any] = ..., auto_shard_dataset: bool = ...) -> None: ...
    def distribute_dataset(self, dataset_fn: Any): ...
    def map(self, map_over: Any, fn: Any, *args: Any, **kwargs: Any): ...
    def configure(self, session_config: Optional[Any] = ..., cluster_spec: Optional[Any] = ..., task_type: Optional[Any] = ..., task_id: Optional[Any] = ...) -> None: ...
    def read_var(self, tower_local_var: Any): ...
    def value_container(self, val: Any): ...
    @property
    def is_single_tower(self): ...
    @property
    def num_towers(self): ...
    @property
    def worker_devices(self): ...
    @property
    def parameter_devices(self): ...
    @property
    def between_graph(self): ...
    @property
    def should_init(self): ...
    @property
    def should_checkpoint(self): ...
    @property
    def should_save_summary(self): ...
    def non_slot_devices(self, var_list: Any): ...
    class _MirroredTowerThread(threading.Thread):
        coord: Any = ...
        distribution: Any = ...
        device: Any = ...
        tower_id: Any = ...
        variable_creator_fn: Any = ...
        main_fn: Any = ...
        main_args: Any = ...
        main_kwargs: Any = ...
        main_result: Any = ...
        done: bool = ...
        merge_fn: Any = ...
        merge_args: Any = ...
        merge_kwargs: Any = ...
        merge_result: Any = ...
        captured_name_scope: Any = ...
        should_run: Any = ...
        has_paused: Any = ...
        context_mode: Any = ...
        context_device_policy: Any = ...
        graph: Any = ...
        def __init__(self, dist: Any, coord: Any, device: Any, variable_creator_fn: Any, fn: Any, *args: Any, **kwargs: Any) -> None: ...
        def run(self) -> None: ...

class MirroredTowerContext(distribute_lib.TowerContext):
    @property
    def device(self): ...
